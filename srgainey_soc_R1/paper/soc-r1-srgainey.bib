

 

@conference{AndrewNg,
author = {Andrew Ng},
title = {{STAIR: The STanford Artificial Intelligence Robot project}},
booktitle = {Triangle Computer Science Distinguished Lecturer Series},
year = {2010},
abstract = {This talk will describe the STAIR home assistant robot project, and the satellite projects that led to key STAIR components such as (i) robotic grasping of previously unknown objects, (ii) depth perception from a single still image, (iii) practical object recognition using multimodal sensors, and (iv) a software architecture for integrative AI.

Since its birth in 1956, the AI dream has been to build systems that exhibit broad-spectrum competence and intelligence.  STAIR revisits this dream, and seeks to integrate onto a single robot platform tools drawn from all areas of AI including learning, vision, navigation, manipulation, planning, and speech/NLP.  This is in distinct contrast to, and also represents an attempt to reverse, the 30 year old trend of working on fragmented AI sub-fields.  STAIR's goal is a useful home assistant robot, and over the long term, we envision a single robot that can perform tasks such as tidying up a room, using a dishwasher, fetching and delivering items, and preparing meals.

In this talk, I'll describe our progress on having the STAIR robot fetch items from around the office, and on having STAIR take inventory of office items.  Specifically, I'll describe: (i) learning to grasp previously unseen objects (including unloading items from a dishwasher); (ii) probabilistic multi-resolution maps, which enable the robot to open/use doors; (iii) a robotic foveal+peripheral vision system for object recognition and tracking.  I'll also outline some of the main technical ideas---such as learning 3-d reconstructions from a single still image, and algorithms for unsupervised feature learning---that played key roles in enabling these STAIR components.
},
URL = {http://www.cs.unc.edu/cms/events/tcsdls/tcsdls-speaker-biographies-and-talk-abstracts-2009-2010},
}

